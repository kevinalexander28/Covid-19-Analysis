{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\kevin\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\kevin\\anaconda3\\lib\\site-packages (1.19.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kevin\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\kevin\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\kevin\\anaconda3\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: sklearn in c:\\users\\kevin\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib) (2020.12.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Install libraries\n",
    "!pip install pandas numpy matplotlib seaborn scipy sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reading datasets\n",
    "# main dataset\n",
    "df = pd.read_csv('../data/covid_19_data.csv',sep=';')\n",
    "# region and climate dataset\n",
    "df_cat = pd.read_csv('../data/covid_19_data_cat.csv')\n",
    "\n",
    "print(\"=======================================================\")\n",
    "print(df)\n",
    "print(\"=======================================================\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of columns and rows\n",
    "print(\"Number of Columns:\", len(df.columns))\n",
    "print(\"Number of Rows:\", len(df))\n",
    "\n",
    "# No. of countries\n",
    "print(\"Number of Countries:\", len(df.country.unique()))\n",
    "\n",
    "# Convert data type\n",
    "df['date'] = df['date'].astype('datetime64[ns]')\n",
    "df['date'] = pd.to_datetime(df['date'], format='%y%m%d').dt.date\n",
    "\n",
    "# No. of days\n",
    "print(\"Number of Days:\", len(df.date.unique()))\n",
    "print(\"Since:\", min(df.date))\n",
    "print(\"Until:\", max(df.date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace some values in the 'country' column so that there are no different names for the same country:\n",
    "'original data','replace with this data':\n",
    "<br>\n",
    "\"('St. Martin',)\",'St. Martin'\n",
    "<br>\n",
    "' Azerbaijan','Azerbaijan'\n",
    "<br>\n",
    "'Cabo Verde','Cape Verde'\n",
    "<br>\n",
    "'Congo (Brazzaville)','Congo'\n",
    "<br>\n",
    "'Congo (Kinshasa)','Congo'\n",
    "<br>\n",
    "'North Ireland','Ireland'\n",
    "<br>\n",
    "'North Macedonia','Macedonia'\n",
    "<br>\n",
    "'occupied Palestinian territory','Palestine'\n",
    "<br>\n",
    "'Holy See','Vatican'\n",
    "<br>\n",
    "'Republic of Ireland','Ireland'\n",
    "<br>\n",
    "'The Bahamas','Bahamas'\n",
    "<br>\n",
    "'The Gambia','Gambia'\n",
    "<br>\n",
    "'Bahamas, The','Bahamas'\n",
    "<br>\n",
    "'Gambia, The','Gambia'\n",
    "<br>\n",
    "'Vatican City','Vatican'\n",
    "<br>\n",
    "'East Timor','Timor-Leste'\n",
    "<br>\n",
    "'West Bank and Gaza','Palestine'\n",
    "<br>\n",
    "'MS Zaandam','Others'\n",
    "<br>\n",
    "'Diamond Princess','Others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with this data\n",
    "replace_country = {\"('St. Martin',)\":'St. Martin',\n",
    "                   ' Azerbaijan':'Azerbaijan',\n",
    "                   'Cabo Verde':'Cape Verde',\n",
    "                   'Congo (Brazzaville)':'Congo',\n",
    "                   'Congo (Kinshasa)':'Congo',\n",
    "                   'North Ireland':'Ireland',\n",
    "                   'North Macedonia':'Macedonia',\n",
    "                   'occupied Palestinian territory':'Palestine',\n",
    "                   'Holy See':'Vatican',\n",
    "                   'Republic of Ireland':'Ireland',\n",
    "                   'The Bahamas':'Bahamas',\n",
    "                   'The Gambia':'Gambia',\n",
    "                   'Bahamas, The':'Bahamas',\n",
    "                   'Gambia, The':'Gambia',\n",
    "                   'Vatican City':'Vatican',\n",
    "                   'East Timor':'Timor-Leste',\n",
    "                   'West Bank and Gaza':'Palestine',\n",
    "                   'MS Zaandam':'Others',\n",
    "                   'Diamond Princess':'Others'\n",
    "                  }\n",
    "\n",
    "df = df.replace({\"country\": replace_country})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New no. of countries\n",
    "print(\"Number of Countries:\", len(df.country.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Several 'countries' on the 'df' dataframe have daily data divided into several 'provinces'. Accumulate the 'confirmed', 'deaths' and 'recovered' data for these provinces so that the daily data for each country is only represented by one row by creating a new dataframe 'df_new'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Daily Data for Each Country\")\n",
    "df_new = df.groupby(by=['country', 'date']).sum().reset_index()\n",
    "print(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop all rows in 'df_new' which data 'confirmed' is below 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Daily Data for Each Country Which Confimed >= 100\")\n",
    "df_new = df_new[df_new['confirmed'] >= 100]\n",
    "print(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add 'region' and 'climate' columns to 'df_new' and fill in the region and climate for each country by referring to 'df_cat' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Add 'region' and 'climate' columns\")\n",
    "df_new = df_new.merge(df_cat, how='left', on=\"country\")\n",
    "print(df_new)\n",
    "df_new.to_csv('covid-aug-20.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a line plot based on dataframe 'df_new' with data 'date' as x and data 'confirmed' as y, where each line represents the accumulative data of each region "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for each region\n",
    "df_asipac = df_new[df_new['region'] == 'Asia & Pacific'].groupby(by=['date']).sum().reset_index()\n",
    "df_europe = df_new[df_new['region'] == 'Europe'].groupby(by=['date']).sum().reset_index()\n",
    "df_arab = df_new[df_new['region'] == 'Arab States'].groupby(by=['date']).sum().reset_index()\n",
    "df_africa = df_new[df_new['region'] == 'Africa'].groupby(by=['date']).sum().reset_index()\n",
    "df_latin = df_new[df_new['region'] == 'South/Latin America'].groupby(by=['date']).sum().reset_index()\n",
    "df_northam = df_new[df_new['region'] == 'North America'].groupby(by=['date']).sum().reset_index()\n",
    "df_mideast = df_new[df_new['region'] == 'Middle east'].groupby(by=['date']).sum().reset_index()\n",
    "\n",
    "# plot based on date\n",
    "plt.plot(df_asipac[\"date\"], df_asipac[\"confirmed\"], label=\"Asia & Pacific\")\n",
    "plt.plot(df_europe[\"date\"], df_europe[\"confirmed\"], label=\"Europe\")\n",
    "plt.plot(df_arab[\"date\"], df_arab[\"confirmed\"], label=\"Arab States\")\n",
    "plt.plot(df_africa[\"date\"], df_africa[\"confirmed\"], label=\"Africa\")\n",
    "plt.plot(df_latin[\"date\"], df_latin[\"confirmed\"], label=\"South/Latin America\")\n",
    "plt.plot(df_northam[\"date\"], df_northam[\"confirmed\"], label=\"North America\")\n",
    "plt.plot(df_mideast[\"date\"], df_mideast[\"confirmed\"], label=\"Middle East\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Data Covid-19 Based on Region from Jan to Aug\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a line plot based on dataframe 'df_new' with data 'date' as x and data 'confirmed' as y, where each line represents the accumulative data of each climate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for each climate\n",
    "df_nontropic = df_new[df_new['climate'] == 'nontropic'].groupby(by=['date']).sum().reset_index()\n",
    "df_tropic = df_new[df_new['climate'] == 'tropic'].groupby(by=['date']).sum().reset_index()\n",
    "\n",
    "# plot based on date\n",
    "plt.plot(df_nontropic[\"date\"], df_nontropic[\"confirmed\"], label=\"nontropic\")\n",
    "plt.plot(df_tropic[\"date\"], df_tropic[\"confirmed\"], label=\"tropic\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Data Covid-19 Based on Climate from Jan to Aug\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe 'df_last' that only contains data from the last date of 'df_new', where each row shows data for 'confirmed', 'deaths', 'recovered', 'region', and 'climate' for each country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get last updated data for each country\n",
    "print(\"Last Updated Data for Each Country\")\n",
    "df_last = df_new.groupby(by=[\"country\"]).max().reset_index()\n",
    "print(df_last)\n",
    "df_last.to_csv('last_update_covid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Based on the df last, identify the top 10 countries with the highest data for 'deaths' and then create the barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the top 10 countries with the highest number of deaths\n",
    "df_10 = df_last.sort_values(by=['deaths'], ascending=False).head(10)\n",
    "print(df_10)\n",
    "\n",
    "# creating the bar plot \n",
    "plt.bar(df_10[\"country\"], df_10[\"deaths\"], color ='maroon', width = 0.4) \n",
    "  \n",
    "plt.xlabel(\"Countries\") \n",
    "plt.ylabel(\"No. of deaths\") \n",
    "plt.title(\"Top 10 Negara dengan Death Tertinggi\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform EDA on the 'df_last' dataframe for the 'confirmed', 'deaths' and 'recovered' columns using a scatter matrix (distinguish the scatter plot colors by region) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# create a scatter matrix for region\n",
    "ax = sns.pairplot(df_last, hue=\"region\")\n",
    "ax.fig.suptitle(\"Scatter Matrix By Region\", y=1.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform EDA on the 'df_last' dataframe for the 'confirmed', 'deaths' and 'recovered' columns using a scatter matrix (distinguish the scatter plot colors by climate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "ax= sns.pairplot(df_last, hue=\"climate\")\n",
    "ax.fig.suptitle(\"Scatter Matrix Berdasarkan Climate\", y=1.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Based on the results in 4.c. and d., what data do visually appear to have a normal distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm the answer to question 5.a. by using the normality test from the scipy module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "# alpha value\n",
    "alpha = 0.05\n",
    "\n",
    "# 'confirmed' normality test\n",
    "print(\"====================confirmed==========================\")\n",
    "stat, p = normaltest(df_last[\"confirmed\"])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "if p > alpha:\n",
    "    print('Normal Distribution (fail to reject H0)')\n",
    "else:\n",
    "    print('Not Normal Distribution (reject H0)')\n",
    "    \n",
    "# 'deaths' normality test\n",
    "print(\"======================deaths===========================\")\n",
    "stat, p = normaltest(df_last[\"deaths\"])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "if p > alpha:\n",
    "    print('Normal Distribution (fail to reject H0)')\n",
    "else:\n",
    "    print('Not Normal Distribution (reject H0)')\n",
    "    \n",
    "# 'recovered' normality test\n",
    "print(\"=====================recovered=========================\")\n",
    "stat, p = normaltest(df_last[\"recovered\"])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "if p > alpha:\n",
    "    print('Normal Distribution (fail to reject H0)')\n",
    "else:\n",
    "    print('Not Normal Distribution (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform an independent t-test to test whether or not a correlation between 'climate' and 'confirmed' in 'df_last'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from scipy import stats\n",
    "\n",
    "# convert data type 'climate'\n",
    "df_last[\"climate\"] = df_last[\"climate\"].astype('category')\n",
    "df_last[\"climate_cat_codes\"] = df_last[\"climate\"].cat.codes\n",
    "\n",
    "# perform independent t-test\n",
    "stat, p = stats.ttest_ind(df_last['confirmed'], df_last['climate_cat_codes'])\n",
    "\n",
    "# hypothesis\n",
    "if p > alpha:\n",
    "    print('fail to reject H0')\n",
    "else:\n",
    "    print('reject H0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If alpha value = 0.05, can the H0 that 'there is no correlation between the climate group and the' Confirmed 'data be rejected? What is the reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a correlation between the climate groups and the data confirmed because the p-value was smaller than the alpha value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a classifier model to predict region of 'X_new' based on the 'confirmed', 'deaths' and 'recovered' data contained in 'df_last'. Show the accuracy of the model using a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import os\n",
    "import graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# path graphviz\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:\\\\Users\\\\kevin\\\\anaconda3\\\\Library\\\\bin\\\\graphviz'\n",
    "\n",
    "# drop na if exists\n",
    "df_last = df_last.dropna()\n",
    "\n",
    "# train-test split\n",
    "X = df_last.iloc[:,2:5]\n",
    "y = df_last[\"region\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# modeling\n",
    "clf = RandomForestClassifier(n_estimators=100, max_features=\"auto\", max_depth=None, \n",
    "                             min_samples_split=2, min_samples_leaf=1, bootstrap=True)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# predict test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# scoring\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import library\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # parameter grid\n",
    "# # Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "# # Number of features to consider at every split\n",
    "# max_features = ['auto', 'sqrt']\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# max_depth.append(None)\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# # Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "\n",
    "# parameters = {\"n_estimators\": n_estimators,\n",
    "#               \"max_features\": max_features,\n",
    "#               \"min_samples_split\": max_depth,\n",
    "#               \"min_samples_leaf\": min_samples_split,\n",
    "#               \"bootstrap\": bootstrap\n",
    "#              }\n",
    "\n",
    "# # parameter optimization\n",
    "# gs = GridSearchCV(clf, param_grid=parameters, cv=2, verbose=1, n_jobs=-1)\n",
    "# gs.fit(X, y)\n",
    "\n",
    "# # best parameter\n",
    "# print(\"Best Param:\", gs.best_params_)\n",
    "# print(\"Best Estimator:\", gs.best_estimator_)\n",
    "# print(\"Best Score:\", gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract single tree\n",
    "estimator = clf.estimators_[0]\n",
    "print(estimator)\n",
    "\n",
    "# visualizing\n",
    "fn = [\"confirmed\", \"deaths\", \"recovered\"]\n",
    "cn = ['AP', 'EU', 'Arab', 'Afr', 'Latin','NA', 'ME']\n",
    "\n",
    "dot_data = export_graphviz(estimator, out_file=None,\n",
    "                           feature_names=fn,\n",
    "                           class_names=cn,\n",
    "                           filled=True, rounded=True,  \n",
    "                           special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph\n",
    "# graph.render('dtree_region',view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict new data\n",
    "X_new=np.array([[1000,30,200],\n",
    "                [2000,40,400],\n",
    "                [50,1,2]])\n",
    "\n",
    "y_pred = clf.predict(X_new)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get importance\n",
    "importance = clf.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a classifier model to predict climate of 'X_new' based on the 'confirmed', 'deaths' and 'recovered' data contained in 'df_last'. Show the accuracy of the model using a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "X = df_last.iloc[:,2:5]\n",
    "y = df_last[\"climate\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# modeling\n",
    "clf = RandomForestClassifier(n_estimators=100, max_features=\"auto\", max_depth=None, \n",
    "                             min_samples_split=2, min_samples_leaf=1, bootstrap=True)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# predict test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# scoring\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import library\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # parameter grid\n",
    "# # Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "# # Number of features to consider at every split\n",
    "# max_features = ['auto', 'sqrt']\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# max_depth.append(None)\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# # Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "\n",
    "# parameters = {\"n_estimators\": n_estimators,\n",
    "#               \"max_features\": max_features,\n",
    "#               \"min_samples_split\": max_depth,\n",
    "#               \"min_samples_leaf\": min_samples_split,\n",
    "#               \"bootstrap\": bootstrap\n",
    "#              }\n",
    "\n",
    "# # parameter optimization\n",
    "# gs = GridSearchCV(clf, param_grid=parameters, cv=5, verbose=1, n_jobs=-1)\n",
    "# gs.fit(X, y)\n",
    "\n",
    "# # best parameter\n",
    "# print(\"Best Param:\", gs.best_params_)\n",
    "# print(\"Best Estimator:\", gs.best_estimator_)\n",
    "# print(\"Best Score:\", gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract single tree\n",
    "estimator = clf.estimators_[0]\n",
    "print(estimator)\n",
    "\n",
    "# visualizing\n",
    "fn = [\"confirmed\", \"deaths\", \"recovered\"]\n",
    "cn = ['nontropic', 'tropic']\n",
    "\n",
    "dot_data = export_graphviz(estimator, out_file=None,\n",
    "                           feature_names=fn,\n",
    "                           class_names=cn,\n",
    "                           filled=True, rounded=True,  \n",
    "                           special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph\n",
    "# graph.render('dtree_climate',view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict new data\n",
    "X_new=np.array([[1000,30,200],\n",
    "                [2000,40,400],\n",
    "                [50,1,2]])\n",
    "\n",
    "y_pred = clf.predict(X_new)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get importance\n",
    "importance = clf.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a linear regression model for the distribution of the number of deaths in the US from 20 March 2020 - 10 August 2020. Plot this regression model. Calculate the R^2 and RMSE values of the model using a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# select date ranges\n",
    "df_lr = df.set_index('date')\n",
    "startdate = pd.to_datetime(\"2020-03-20\").date()\n",
    "enddate = pd.to_datetime(\"2020-08-10\").date()\n",
    "df_lr = df_lr.loc[startdate : enddate].reset_index()\n",
    "\n",
    "# train-test split\n",
    "X = df_lr[['confirmed']]\n",
    "y = df_lr[\"deaths\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# modeling\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "print(\"Intercept:\", regressor.intercept_)\n",
    "print(\"Coefficient:\", regressor.coef_)\n",
    "\n",
    "# predict test data\n",
    "y_pred = regressor.predict(X_test)\n",
    "df_pred = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "\n",
    "# scoring\n",
    "print('Mean Absolute Error:', mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print('R^2 Score:', r2_score(y_test, y_pred))\n",
    "\n",
    "# visualizing\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.plot(X_test, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a clustering model with 5 clusters (cluster 0-4) for the 'Z' array. Predict the cluster numbers based on 'confirmed', 'deaths' and 'recovered' data for 'df_last' for the following countries:\n",
    "<br>\n",
    "a. Indonesia 4\n",
    "<br>\n",
    "b. Singapore 4\n",
    "<br>\n",
    "c. US 1\n",
    "<br>\n",
    "d. Italy 4\n",
    "<br>\n",
    "e. Iran 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# data\n",
    "Z=df_last.loc[:,['confirmed','recovered','deaths']].values\n",
    "Z[:5]\n",
    "\n",
    "# normalization\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(Z)\n",
    "\n",
    "# modeling\n",
    "kmeans = KMeans(init=\"random\", n_clusters=5, n_init=10, max_iter=300, random_state=42)\n",
    "kmeans.fit(scaled_features)\n",
    "print(kmeans.labels_[:5])\n",
    "\n",
    "# scoring\n",
    "kmeans_silhouette = silhouette_score(scaled_features, kmeans.labels_).round(2)\n",
    "print(\"Silhouette Score:\", kmeans_silhouette)\n",
    "\n",
    "# predict new data\n",
    "df_indonesia = df_last.loc[df_last['country'] == \"Indonesia\"].iloc[:,2:5]\n",
    "print(df_indonesia)\n",
    "\n",
    "df_singapore = df_last.loc[df_last['country'] == \"Singapore\"].iloc[:,2:5]\n",
    "print(df_singapore)\n",
    "\n",
    "df_us = df_last.loc[df_last['country'] == \"US\"].iloc[:,2:5]\n",
    "print(df_us)\n",
    "\n",
    "df_italy = df_last.loc[df_last['country'] == \"Italy\"].iloc[:,2:5]\n",
    "print(df_italy)\n",
    "\n",
    "df_iran = df_last.loc[df_last['country'] == \"Iran\"].iloc[:,2:5]\n",
    "print(df_iran)\n",
    "\n",
    "df_new_pred_cluster = pd.concat([df_indonesia, df_singapore, df_us, df_italy, df_iran])\n",
    "\n",
    "X = StandardScaler().fit_transform(df_new_pred_cluster)\n",
    "y_pred = kmeans.predict(X)\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
